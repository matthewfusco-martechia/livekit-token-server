import { EventEmitter } from "node:events";
import { AsyncIterableQueue } from "../utils.js";
var LLMEvent = /* @__PURE__ */ ((LLMEvent2) => {
  LLMEvent2[LLMEvent2["METRICS_COLLECTED"] = 0] = "METRICS_COLLECTED";
  return LLMEvent2;
})(LLMEvent || {});
class LLM extends EventEmitter {
}
class LLMStream {
  output = new AsyncIterableQueue();
  queue = new AsyncIterableQueue();
  closed = false;
  _functionCalls = [];
  #llm;
  #chatCtx;
  #fncCtx;
  constructor(llm, chatCtx, fncCtx) {
    this.#llm = llm;
    this.#chatCtx = chatCtx;
    this.#fncCtx = fncCtx;
    this.monitorMetrics();
  }
  async monitorMetrics() {
    const startTime = process.hrtime.bigint();
    let ttft;
    let requestId = "";
    let usage;
    for await (const ev of this.queue) {
      this.output.put(ev);
      requestId = ev.requestId;
      if (!ttft) {
        ttft = process.hrtime.bigint() - startTime;
      }
      if (ev.usage) {
        usage = ev.usage;
      }
    }
    this.output.close();
    const duration = process.hrtime.bigint() - startTime;
    const metrics = {
      timestamp: Date.now(),
      requestId,
      ttft: Math.trunc(Number(ttft / BigInt(1e6))),
      duration: Math.trunc(Number(duration / BigInt(1e6))),
      cancelled: false,
      // XXX(nbsp)
      label: this.label,
      completionTokens: (usage == null ? void 0 : usage.completionTokens) || 0,
      promptTokens: (usage == null ? void 0 : usage.promptTokens) || 0,
      totalTokens: (usage == null ? void 0 : usage.totalTokens) || 0,
      tokensPerSecond: ((usage == null ? void 0 : usage.completionTokens) || 0) / Math.trunc(Number(duration / BigInt(1e9)))
    };
    this.#llm.emit(0 /* METRICS_COLLECTED */, metrics);
  }
  /** List of called functions from this stream. */
  get functionCalls() {
    return this._functionCalls;
  }
  /** The function context of this stream. */
  get fncCtx() {
    return this.#fncCtx;
  }
  /** The initial chat context of this stream. */
  get chatCtx() {
    return this.#chatCtx;
  }
  /** Execute all deferred functions of this stream concurrently. */
  executeFunctions() {
    this._functionCalls.forEach(
      (f) => f.task = f.func.execute(f.params).then(
        (result) => ({ name: f.name, toolCallId: f.toolCallId, result }),
        (error) => ({ name: f.name, toolCallId: f.toolCallId, error })
      )
    );
    return this._functionCalls;
  }
  next() {
    return this.output.next();
  }
  close() {
    this.output.close();
    this.queue.close();
    this.closed = true;
  }
  [Symbol.asyncIterator]() {
    return this;
  }
}
export {
  LLM,
  LLMEvent,
  LLMStream
};
//# sourceMappingURL=llm.js.map