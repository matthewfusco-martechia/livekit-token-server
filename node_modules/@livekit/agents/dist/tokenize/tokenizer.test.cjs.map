{"version":3,"sources":["../../src/tokenize/tokenizer.test.ts"],"sourcesContent":["// SPDX-FileCopyrightText: 2024 LiveKit, Inc.\n//\n// SPDX-License-Identifier: Apache-2.0\nimport { describe, expect, it } from 'vitest';\nimport { SentenceTokenizer, WordTokenizer, hyphenateWord } from './basic/index.js';\nimport { splitParagraphs } from './basic/paragraph.js';\n\nconst TEXT =\n  'Hi! ' +\n  'LiveKit is a platform for live audio and video applications and services. ' +\n  'R.T.C stands for Real-Time Communication... again R.T.C. ' +\n  'Mr. Theo is testing the sentence tokenizer. ' +\n  'This is a test. Another test. ' +\n  'A short sentence. ' +\n  'A longer sentence that is longer than the previous sentence. ' +\n  'f(x) = x * 2.54 + 42. ' +\n  'Hey! Hi! Hello! ';\n\nconst EXPECTED_MIN_20 = [\n  'Hi! LiveKit is a platform for live audio and video applications and services.',\n  'R.T.C stands for Real-Time Communication... again R.T.C.',\n  'Mr. Theo is testing the sentence tokenizer.',\n  'This is a test. Another test.',\n  'A short sentence. A longer sentence that is longer than the previous sentence.',\n  'f(x) = x * 2.54 + 42.',\n  'Hey! Hi! Hello!',\n];\n\nconst WORDS_TEXT = 'This is a test. Blabla another test! multiple consecutive spaces:     done';\nconst WORDS_EXPECTED = [\n  'This',\n  'is',\n  'a',\n  'test',\n  'Blabla',\n  'another',\n  'test',\n  'multiple',\n  'consecutive',\n  'spaces',\n  'done',\n];\n\nconst WORDS_PUNCT_TEXT =\n  'This is <phoneme alphabet=\"cmu-arpabet\" ph=\"AE K CH UW AH L IY\">actually</phoneme> tricky to handle.';\nconst WORDS_PUNCT_EXPECTED = [\n  'This',\n  'is',\n  '<phoneme',\n  'alphabet=\"cmu-arpabet\"',\n  'ph=\"AE',\n  'K',\n  'CH',\n  'UW',\n  'AH',\n  'L',\n  'IY\">actually</phoneme>',\n  'tricky',\n  'to',\n  'handle.',\n];\n\nconst HYPHENATOR_TEXT = ['Segment', 'expected', 'communication', 'window', 'welcome', 'bedroom'];\nconst HYPHENATOR_EXPECTED = [\n  ['Seg', 'ment'],\n  ['ex', 'pect', 'ed'],\n  ['com', 'mu', 'ni', 'ca', 'tion'],\n  ['win', 'dow'],\n  ['wel', 'come'],\n  ['bed', 'room'],\n];\n\nconst PARAGRAPH_TEST_CASES: [string, [string, number, number][]][] = [\n  ['Single paragraph.', [['Single paragraph.', 0, 17]]],\n  [\n    'Paragraph 1.\\n\\nParagraph 2.',\n    [\n      ['Paragraph 1.', 0, 12],\n      ['Paragraph 2.', 14, 26],\n    ],\n  ],\n  [\n    'Para 1.\\n\\nPara 2.\\n\\nPara 3.',\n    [\n      ['Para 1.', 0, 7],\n      ['Para 2.', 9, 16],\n      ['Para 3.', 18, 25],\n    ],\n  ],\n  ['\\n\\nParagraph with leading newlines.', [['Paragraph with leading newlines.', 2, 34]]],\n  ['Paragraph with trailing newlines.\\n\\n', [['Paragraph with trailing newlines.', 0, 33]]],\n  [\n    '\\n\\n  Paragraph with leading and trailing spaces.  \\n\\n',\n    [['Paragraph with leading and trailing spaces.', 4, 47]],\n  ],\n  [\n    'Para 1.\\n\\n\\n\\nPara 2.', // Multiple newlines between paragraphs\n    [\n      ['Para 1.', 0, 7],\n      ['Para 2.', 11, 18],\n    ],\n  ],\n  [\n    'Para 1.\\n \\n \\nPara 2.', // Newlines with spaces between paragraphs\n    [\n      ['Para 1.', 0, 7],\n      ['Para 2.', 12, 19],\n    ],\n  ],\n  [\n    '', // Empty string\n    [],\n  ],\n  [\n    '\\n\\n\\n', // Only newlines\n    [],\n  ],\n  [\n    'Line 1\\nLine 2\\nLine 3', // Single paragraph with newlines\n    [['Line 1\\nLine 2\\nLine 3', 0, 20]],\n  ],\n];\n\ndescribe('tokenizer', () => {\n  describe('SentenceTokenizer', () => {\n    const tokenizer = new SentenceTokenizer();\n\n    it('should tokenize sentences correctly', () => {\n      expect(tokenizer.tokenize(TEXT).every((x, i) => EXPECTED_MIN_20[i] === x)).toBeTruthy();\n    });\n\n    it('should stream tokenize sentences correctly', async () => {\n      const pattern = [1, 2, 4];\n      let text = TEXT;\n      const chunks = [];\n      const patternIter = Array(Math.ceil(text.length / pattern.reduce((sum, num) => sum + num, 0)))\n        .fill(pattern)\n        .flat()\n        [Symbol.iterator]();\n\n      for (const size of patternIter) {\n        if (!text) break;\n        chunks.push(text.slice(undefined, size));\n        text = text.slice(size);\n      }\n      const stream = tokenizer.stream();\n      for (const chunk of chunks) {\n        stream.pushText(chunk);\n      }\n      stream.endInput();\n      stream.close();\n\n      for (const x of EXPECTED_MIN_20) {\n        await stream.next().then((value) => {\n          if (value.value) {\n            expect(value.value.token).toStrictEqual(x);\n          }\n        });\n      }\n    });\n  });\n  describe('WordTokenizer', () => {\n    const tokenizer = new WordTokenizer();\n\n    it('should tokenize words correctly', () => {\n      expect(tokenizer.tokenize(WORDS_TEXT).every((x, i) => WORDS_EXPECTED[i] === x)).toBeTruthy();\n    });\n\n    it('should stream tokenize words correctly', async () => {\n      const pattern = [1, 2, 4];\n      let text = WORDS_TEXT;\n      const chunks = [];\n      const patternIter = Array(Math.ceil(text.length / pattern.reduce((sum, num) => sum + num, 0)))\n        .fill(pattern)\n        .flat()\n        [Symbol.iterator]();\n\n      for (const size of patternIter) {\n        if (!text) break;\n        chunks.push(text.slice(undefined, size));\n        text = text.slice(size);\n      }\n      const stream = tokenizer.stream();\n      for (const chunk of chunks) {\n        stream.pushText(chunk);\n      }\n      stream.endInput();\n      stream.close();\n\n      for (const x of WORDS_EXPECTED) {\n        await stream.next().then((value) => {\n          if (value.value) {\n            expect(value.value.token).toStrictEqual(x);\n          }\n        });\n      }\n    });\n\n    describe('punctuation handling', () => {\n      const tokenizerPunct = new WordTokenizer(false);\n\n      it('should tokenize words correctly', () => {\n        expect(\n          tokenizerPunct.tokenize(WORDS_PUNCT_TEXT).every((x, i) => WORDS_PUNCT_EXPECTED[i] === x),\n        ).toBeTruthy();\n      });\n\n      it('should stream tokenize words correctly', async () => {\n        const pattern = [1, 2, 4];\n        let text = WORDS_PUNCT_TEXT;\n        const chunks = [];\n        const patternIter = Array(\n          Math.ceil(text.length / pattern.reduce((sum, num) => sum + num, 0)),\n        )\n          .fill(pattern)\n          .flat()\n          [Symbol.iterator]();\n\n        for (const size of patternIter) {\n          if (!text) break;\n          chunks.push(text.slice(undefined, size));\n          text = text.slice(size);\n        }\n        const stream = tokenizerPunct.stream();\n        for (const chunk of chunks) {\n          stream.pushText(chunk);\n        }\n        stream.endInput();\n        stream.close();\n\n        for (const x of WORDS_PUNCT_EXPECTED) {\n          await stream.next().then((value) => {\n            if (value.value) {\n              expect(value.value.token).toStrictEqual(x);\n            }\n          });\n        }\n      });\n    });\n  });\n  describe('hyphenateWord', () => {\n    it('should hyphenate correctly', () => {\n      HYPHENATOR_TEXT.forEach((x, i) => {\n        expect(hyphenateWord(x)).toStrictEqual(HYPHENATOR_EXPECTED[i]);\n      });\n    });\n  });\n  describe('splitParagraphs', () => {\n    it('should tokenize paragraphs correctly', () => {\n      PARAGRAPH_TEST_CASES.forEach(([a, b]) => {\n        expect(splitParagraphs(a)).toStrictEqual(b);\n      });\n    });\n  });\n});\n"],"mappings":";AAGA,oBAAqC;AACrC,mBAAgE;AAChE,uBAAgC;AAEhC,MAAM,OACJ;AAUF,MAAM,kBAAkB;AAAA,EACtB;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACF;AAEA,MAAM,aAAa;AACnB,MAAM,iBAAiB;AAAA,EACrB;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACF;AAEA,MAAM,mBACJ;AACF,MAAM,uBAAuB;AAAA,EAC3B;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACF;AAEA,MAAM,kBAAkB,CAAC,WAAW,YAAY,iBAAiB,UAAU,WAAW,SAAS;AAC/F,MAAM,sBAAsB;AAAA,EAC1B,CAAC,OAAO,MAAM;AAAA,EACd,CAAC,MAAM,QAAQ,IAAI;AAAA,EACnB,CAAC,OAAO,MAAM,MAAM,MAAM,MAAM;AAAA,EAChC,CAAC,OAAO,KAAK;AAAA,EACb,CAAC,OAAO,MAAM;AAAA,EACd,CAAC,OAAO,MAAM;AAChB;AAEA,MAAM,uBAA+D;AAAA,EACnE,CAAC,qBAAqB,CAAC,CAAC,qBAAqB,GAAG,EAAE,CAAC,CAAC;AAAA,EACpD;AAAA,IACE;AAAA,IACA;AAAA,MACE,CAAC,gBAAgB,GAAG,EAAE;AAAA,MACtB,CAAC,gBAAgB,IAAI,EAAE;AAAA,IACzB;AAAA,EACF;AAAA,EACA;AAAA,IACE;AAAA,IACA;AAAA,MACE,CAAC,WAAW,GAAG,CAAC;AAAA,MAChB,CAAC,WAAW,GAAG,EAAE;AAAA,MACjB,CAAC,WAAW,IAAI,EAAE;AAAA,IACpB;AAAA,EACF;AAAA,EACA,CAAC,wCAAwC,CAAC,CAAC,oCAAoC,GAAG,EAAE,CAAC,CAAC;AAAA,EACtF,CAAC,yCAAyC,CAAC,CAAC,qCAAqC,GAAG,EAAE,CAAC,CAAC;AAAA,EACxF;AAAA,IACE;AAAA,IACA,CAAC,CAAC,+CAA+C,GAAG,EAAE,CAAC;AAAA,EACzD;AAAA,EACA;AAAA,IACE;AAAA;AAAA,IACA;AAAA,MACE,CAAC,WAAW,GAAG,CAAC;AAAA,MAChB,CAAC,WAAW,IAAI,EAAE;AAAA,IACpB;AAAA,EACF;AAAA,EACA;AAAA,IACE;AAAA;AAAA,IACA;AAAA,MACE,CAAC,WAAW,GAAG,CAAC;AAAA,MAChB,CAAC,WAAW,IAAI,EAAE;AAAA,IACpB;AAAA,EACF;AAAA,EACA;AAAA,IACE;AAAA;AAAA,IACA,CAAC;AAAA,EACH;AAAA,EACA;AAAA,IACE;AAAA;AAAA,IACA,CAAC;AAAA,EACH;AAAA,EACA;AAAA,IACE;AAAA;AAAA,IACA,CAAC,CAAC,0BAA0B,GAAG,EAAE,CAAC;AAAA,EACpC;AACF;AAAA,IAEA,wBAAS,aAAa,MAAM;AAC1B,8BAAS,qBAAqB,MAAM;AAClC,UAAM,YAAY,IAAI,+BAAkB;AAExC,0BAAG,uCAAuC,MAAM;AAC9C,gCAAO,UAAU,SAAS,IAAI,EAAE,MAAM,CAAC,GAAG,MAAM,gBAAgB,CAAC,MAAM,CAAC,CAAC,EAAE,WAAW;AAAA,IACxF,CAAC;AAED,0BAAG,8CAA8C,YAAY;AAC3D,YAAM,UAAU,CAAC,GAAG,GAAG,CAAC;AACxB,UAAI,OAAO;AACX,YAAM,SAAS,CAAC;AAChB,YAAM,cAAc,MAAM,KAAK,KAAK,KAAK,SAAS,QAAQ,OAAO,CAAC,KAAK,QAAQ,MAAM,KAAK,CAAC,CAAC,CAAC,EAC1F,KAAK,OAAO,EACZ,KAAK,EACL,OAAO,QAAQ,EAAE;AAEpB,iBAAW,QAAQ,aAAa;AAC9B,YAAI,CAAC,KAAM;AACX,eAAO,KAAK,KAAK,MAAM,QAAW,IAAI,CAAC;AACvC,eAAO,KAAK,MAAM,IAAI;AAAA,MACxB;AACA,YAAM,SAAS,UAAU,OAAO;AAChC,iBAAW,SAAS,QAAQ;AAC1B,eAAO,SAAS,KAAK;AAAA,MACvB;AACA,aAAO,SAAS;AAChB,aAAO,MAAM;AAEb,iBAAW,KAAK,iBAAiB;AAC/B,cAAM,OAAO,KAAK,EAAE,KAAK,CAAC,UAAU;AAClC,cAAI,MAAM,OAAO;AACf,sCAAO,MAAM,MAAM,KAAK,EAAE,cAAc,CAAC;AAAA,UAC3C;AAAA,QACF,CAAC;AAAA,MACH;AAAA,IACF,CAAC;AAAA,EACH,CAAC;AACD,8BAAS,iBAAiB,MAAM;AAC9B,UAAM,YAAY,IAAI,2BAAc;AAEpC,0BAAG,mCAAmC,MAAM;AAC1C,gCAAO,UAAU,SAAS,UAAU,EAAE,MAAM,CAAC,GAAG,MAAM,eAAe,CAAC,MAAM,CAAC,CAAC,EAAE,WAAW;AAAA,IAC7F,CAAC;AAED,0BAAG,0CAA0C,YAAY;AACvD,YAAM,UAAU,CAAC,GAAG,GAAG,CAAC;AACxB,UAAI,OAAO;AACX,YAAM,SAAS,CAAC;AAChB,YAAM,cAAc,MAAM,KAAK,KAAK,KAAK,SAAS,QAAQ,OAAO,CAAC,KAAK,QAAQ,MAAM,KAAK,CAAC,CAAC,CAAC,EAC1F,KAAK,OAAO,EACZ,KAAK,EACL,OAAO,QAAQ,EAAE;AAEpB,iBAAW,QAAQ,aAAa;AAC9B,YAAI,CAAC,KAAM;AACX,eAAO,KAAK,KAAK,MAAM,QAAW,IAAI,CAAC;AACvC,eAAO,KAAK,MAAM,IAAI;AAAA,MACxB;AACA,YAAM,SAAS,UAAU,OAAO;AAChC,iBAAW,SAAS,QAAQ;AAC1B,eAAO,SAAS,KAAK;AAAA,MACvB;AACA,aAAO,SAAS;AAChB,aAAO,MAAM;AAEb,iBAAW,KAAK,gBAAgB;AAC9B,cAAM,OAAO,KAAK,EAAE,KAAK,CAAC,UAAU;AAClC,cAAI,MAAM,OAAO;AACf,sCAAO,MAAM,MAAM,KAAK,EAAE,cAAc,CAAC;AAAA,UAC3C;AAAA,QACF,CAAC;AAAA,MACH;AAAA,IACF,CAAC;AAED,gCAAS,wBAAwB,MAAM;AACrC,YAAM,iBAAiB,IAAI,2BAAc,KAAK;AAE9C,4BAAG,mCAAmC,MAAM;AAC1C;AAAA,UACE,eAAe,SAAS,gBAAgB,EAAE,MAAM,CAAC,GAAG,MAAM,qBAAqB,CAAC,MAAM,CAAC;AAAA,QACzF,EAAE,WAAW;AAAA,MACf,CAAC;AAED,4BAAG,0CAA0C,YAAY;AACvD,cAAM,UAAU,CAAC,GAAG,GAAG,CAAC;AACxB,YAAI,OAAO;AACX,cAAM,SAAS,CAAC;AAChB,cAAM,cAAc;AAAA,UAClB,KAAK,KAAK,KAAK,SAAS,QAAQ,OAAO,CAAC,KAAK,QAAQ,MAAM,KAAK,CAAC,CAAC;AAAA,QACpE,EACG,KAAK,OAAO,EACZ,KAAK,EACL,OAAO,QAAQ,EAAE;AAEpB,mBAAW,QAAQ,aAAa;AAC9B,cAAI,CAAC,KAAM;AACX,iBAAO,KAAK,KAAK,MAAM,QAAW,IAAI,CAAC;AACvC,iBAAO,KAAK,MAAM,IAAI;AAAA,QACxB;AACA,cAAM,SAAS,eAAe,OAAO;AACrC,mBAAW,SAAS,QAAQ;AAC1B,iBAAO,SAAS,KAAK;AAAA,QACvB;AACA,eAAO,SAAS;AAChB,eAAO,MAAM;AAEb,mBAAW,KAAK,sBAAsB;AACpC,gBAAM,OAAO,KAAK,EAAE,KAAK,CAAC,UAAU;AAClC,gBAAI,MAAM,OAAO;AACf,wCAAO,MAAM,MAAM,KAAK,EAAE,cAAc,CAAC;AAAA,YAC3C;AAAA,UACF,CAAC;AAAA,QACH;AAAA,MACF,CAAC;AAAA,IACH,CAAC;AAAA,EACH,CAAC;AACD,8BAAS,iBAAiB,MAAM;AAC9B,0BAAG,8BAA8B,MAAM;AACrC,sBAAgB,QAAQ,CAAC,GAAG,MAAM;AAChC,sCAAO,4BAAc,CAAC,CAAC,EAAE,cAAc,oBAAoB,CAAC,CAAC;AAAA,MAC/D,CAAC;AAAA,IACH,CAAC;AAAA,EACH,CAAC;AACD,8BAAS,mBAAmB,MAAM;AAChC,0BAAG,wCAAwC,MAAM;AAC/C,2BAAqB,QAAQ,CAAC,CAAC,GAAG,CAAC,MAAM;AACvC,sCAAO,kCAAgB,CAAC,CAAC,EAAE,cAAc,CAAC;AAAA,MAC5C,CAAC;AAAA,IACH,CAAC;AAAA,EACH,CAAC;AACH,CAAC;","names":[]}