"use strict";
var __create = Object.create;
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __getProtoOf = Object.getPrototypeOf;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toESM = (mod, isNodeMode, target) => (target = mod != null ? __create(__getProtoOf(mod)) : {}, __copyProps(
  // If the importer is in node compatibility mode or this is not an ESM
  // file that has been converted to a CommonJS file using a Babel-
  // compatible transform (i.e. "__esModule" has not been set), then set
  // "default" to the CommonJS "module.exports" for node compatibility.
  isNodeMode || !mod || !mod.__esModule ? __defProp(target, "default", { value: mod, enumerable: true }) : target,
  mod
));
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);
var multimodal_agent_exports = {};
__export(multimodal_agent_exports, {
  AGENT_STATE_ATTRIBUTE: () => AGENT_STATE_ATTRIBUTE,
  MultimodalAgent: () => MultimodalAgent,
  RealtimeModel: () => RealtimeModel,
  RealtimeSession: () => RealtimeSession
});
module.exports = __toCommonJS(multimodal_agent_exports);
var import_rtc_node = require("@livekit/rtc-node");
var import_node_events = require("node:events");
var import_audio = require("../audio.cjs");
var llm = __toESM(require("../llm/index.cjs"), 1);
var import_log = require("../log.cjs");
var import_transcription = require("../transcription.cjs");
var import_utils = require("../utils.cjs");
var import_agent_playout = require("./agent_playout.cjs");
class RealtimeSession extends import_node_events.EventEmitter {
}
class RealtimeModel {
}
const AGENT_STATE_ATTRIBUTE = "lk.agent.state";
class MultimodalAgent extends import_node_events.EventEmitter {
  model;
  room = null;
  linkedParticipant = null;
  subscribedTrack = null;
  readMicroTask = null;
  #textResponseRetries = 0;
  #maxTextResponseRetries;
  constructor({
    model,
    chatCtx,
    fncCtx,
    maxTextResponseRetries = 5
  }) {
    super();
    this.model = model;
    this.#chatCtx = chatCtx;
    this.#fncCtx = fncCtx;
    this.#maxTextResponseRetries = maxTextResponseRetries;
  }
  #participant = null;
  #agentPublication = null;
  #localTrackSid = null;
  #localSource = null;
  #agentPlayout = null;
  #playingHandle = void 0;
  #logger = (0, import_log.log)();
  #session = null;
  #fncCtx = void 0;
  #chatCtx = void 0;
  #_started = false;
  #_pendingFunctionCalls = /* @__PURE__ */ new Set();
  #_speaking = false;
  get fncCtx() {
    return this.#fncCtx;
  }
  set fncCtx(ctx) {
    this.#fncCtx = ctx;
    if (this.#session) {
      this.#session.fncCtx = ctx;
    }
  }
  get #pendingFunctionCalls() {
    return this.#_pendingFunctionCalls;
  }
  set #pendingFunctionCalls(calls) {
    this.#_pendingFunctionCalls = calls;
    this.#updateState();
  }
  get #speaking() {
    return this.#_speaking;
  }
  set #speaking(isSpeaking) {
    this.#_speaking = isSpeaking;
    this.#updateState();
  }
  get #started() {
    return this.#_started;
  }
  set #started(started) {
    this.#_started = started;
    this.#updateState();
  }
  start(room, participant = null) {
    return new Promise(async (resolve, reject) => {
      var _a;
      if (this.#started) {
        reject(new Error("MultimodalAgent already started"));
      }
      this.#updateState();
      room.on(import_rtc_node.RoomEvent.ParticipantConnected, (participant2) => {
        if (this.linkedParticipant) {
          return;
        }
        this.#linkParticipant(participant2.identity);
      });
      room.on(
        import_rtc_node.RoomEvent.TrackPublished,
        (trackPublication, participant2) => {
          if (this.linkedParticipant && participant2.identity === this.linkedParticipant.identity && trackPublication.source === import_rtc_node.TrackSource.SOURCE_MICROPHONE && !trackPublication.subscribed) {
            trackPublication.setSubscribed(true);
          }
        }
      );
      room.on(import_rtc_node.RoomEvent.TrackSubscribed, this.#handleTrackSubscription.bind(this));
      this.room = room;
      this.#participant = participant;
      this.#localSource = new import_rtc_node.AudioSource(this.model.sampleRate, this.model.numChannels);
      this.#agentPlayout = new import_agent_playout.AgentPlayout(
        this.#localSource,
        this.model.sampleRate,
        this.model.numChannels,
        this.model.inFrameSize,
        this.model.outFrameSize
      );
      const onPlayoutStarted = () => {
        this.emit("agent_started_speaking");
        this.#speaking = true;
      };
      const onPlayoutStopped = (interrupted) => {
        this.emit("agent_stopped_speaking");
        this.#speaking = false;
        if (this.#playingHandle) {
          let text = this.#playingHandle.synchronizer.playedText;
          if (interrupted) {
            text += "\u2026";
          }
          const msg = llm.ChatMessage.create({
            role: llm.ChatRole.ASSISTANT,
            text
          });
          if (interrupted) {
            this.emit("agent_speech_interrupted", msg);
          } else {
            this.emit("agent_speech_committed", msg);
          }
          this.#logger.child({ transcription: text, interrupted }).debug("committed agent speech");
        }
      };
      this.#agentPlayout.on("playout_started", onPlayoutStarted);
      this.#agentPlayout.on("playout_stopped", onPlayoutStopped);
      const track = import_rtc_node.LocalAudioTrack.createAudioTrack("assistant_voice", this.#localSource);
      const options = new import_rtc_node.TrackPublishOptions();
      options.source = import_rtc_node.TrackSource.SOURCE_MICROPHONE;
      this.#agentPublication = await ((_a = room.localParticipant) == null ? void 0 : _a.publishTrack(track, options)) || null;
      if (!this.#agentPublication) {
        this.#logger.error("Failed to publish track");
        reject(new Error("Failed to publish track"));
        return;
      }
      await this.#agentPublication.waitForSubscription();
      if (participant) {
        if (typeof participant === "string") {
          this.#linkParticipant(participant);
        } else {
          this.#linkParticipant(participant.identity);
        }
      } else {
        for (const participant2 of room.remoteParticipants.values()) {
          this.#linkParticipant(participant2.identity);
          break;
        }
      }
      this.#session = this.model.session({ fncCtx: this.#fncCtx, chatCtx: this.#chatCtx });
      this.#started = true;
      this.#session.on("response_content_added", (message) => {
        var _a2;
        if (message.contentType === "text") return;
        const synchronizer = new import_transcription.TextAudioSynchronizer(import_transcription.defaultTextSyncOptions);
        synchronizer.on("textUpdated", (text) => {
          this.#publishTranscription(
            this.room.localParticipant.identity,
            this.#getLocalTrackSid(),
            text.text,
            text.final,
            text.id
          );
        });
        const handle = (_a2 = this.#agentPlayout) == null ? void 0 : _a2.play(
          message.itemId,
          message.contentIndex,
          synchronizer,
          message.textStream,
          message.audioStream
        );
        this.#playingHandle = handle;
      });
      this.#session.on("response_content_done", (message) => {
        if (message.contentType === "text") {
          if (this.#textResponseRetries >= this.#maxTextResponseRetries) {
            throw new Error(
              `The OpenAI Realtime API returned a text response after ${this.#maxTextResponseRetries} retries. Please try to reduce the number of text system or assistant messages in the chat context.`
            );
          }
          this.#textResponseRetries++;
          this.#logger.child({
            itemId: message.itemId,
            text: message.text,
            retries: this.#textResponseRetries
          }).warn(
            "The OpenAI Realtime API returned a text response instead of audio. Attempting to recover to audio mode..."
          );
          this.#session.recoverFromTextResponse(message.itemId);
        } else {
          this.#textResponseRetries = 0;
        }
      });
      this.#session.on("input_speech_committed", (ev) => {
        var _a2, _b;
        const participantIdentity = (_a2 = this.linkedParticipant) == null ? void 0 : _a2.identity;
        const trackSid = (_b = this.subscribedTrack) == null ? void 0 : _b.sid;
        if (participantIdentity && trackSid) {
          this.#publishTranscription(participantIdentity, trackSid, "\u2026", false, ev.itemId);
        } else {
          this.#logger.error("Participant or track not set");
        }
      });
      this.#session.on("input_speech_transcription_completed", (ev) => {
        var _a2, _b;
        const transcription = ev.transcript;
        const participantIdentity = (_a2 = this.linkedParticipant) == null ? void 0 : _a2.identity;
        const trackSid = (_b = this.subscribedTrack) == null ? void 0 : _b.sid;
        if (participantIdentity && trackSid) {
          this.#publishTranscription(participantIdentity, trackSid, transcription, true, ev.itemId);
        } else {
          this.#logger.error("Participant or track not set");
        }
        const userMsg = llm.ChatMessage.create({
          role: llm.ChatRole.USER,
          text: transcription
        });
        this.emit("user_speech_committed", userMsg);
        this.#logger.child({ transcription }).debug("committed user speech");
      });
      this.#session.on("input_speech_started", (ev) => {
        var _a2, _b;
        this.emit("user_started_speaking");
        if (this.#playingHandle && !this.#playingHandle.done) {
          this.#playingHandle.interrupt();
          this.#session.conversation.item.truncate(
            this.#playingHandle.itemId,
            this.#playingHandle.contentIndex,
            Math.floor(this.#playingHandle.audioSamples / 24e3 * 1e3)
          );
          this.#playingHandle = void 0;
        }
        const participantIdentity = (_a2 = this.linkedParticipant) == null ? void 0 : _a2.identity;
        const trackSid = (_b = this.subscribedTrack) == null ? void 0 : _b.sid;
        if (participantIdentity && trackSid) {
          this.#publishTranscription(participantIdentity, trackSid, "\u2026", false, ev.itemId);
        }
      });
      this.#session.on("input_speech_stopped", (ev) => {
        this.emit("user_stopped_speaking");
      });
      this.#session.on("function_call_started", (ev) => {
        this.#pendingFunctionCalls.add(ev.callId);
        this.#updateState();
      });
      this.#session.on("function_call_completed", (ev) => {
        this.#pendingFunctionCalls.delete(ev.callId);
        this.#updateState();
      });
      this.#session.on("function_call_failed", (ev) => {
        this.#pendingFunctionCalls.delete(ev.callId);
        this.#updateState();
      });
      this.#session.on("metrics_collected", (metrics) => {
        this.emit("metrics_collected", metrics);
      });
      resolve(this.#session);
    });
  }
  #linkParticipant(participantIdentity) {
    if (!this.room) {
      this.#logger.error("Room is not set");
      return;
    }
    this.linkedParticipant = this.room.remoteParticipants.get(participantIdentity) || null;
    if (!this.linkedParticipant) {
      this.#logger.error(`Participant with identity ${participantIdentity} not found`);
      return;
    }
    if (this.linkedParticipant.trackPublications.size > 0) {
      this.#subscribeToMicrophone();
    }
    for (const publication of this.linkedParticipant.trackPublications.values()) {
      if (publication.source === import_rtc_node.TrackSource.SOURCE_MICROPHONE && publication.track) {
        this.#handleTrackSubscription(publication.track, publication, this.linkedParticipant);
        break;
      }
    }
  }
  #subscribeToMicrophone() {
    if (!this.linkedParticipant) {
      this.#logger.error("Participant is not set");
      return;
    }
    let microphonePublication = void 0;
    for (const publication of this.linkedParticipant.trackPublications.values()) {
      if (publication.source === import_rtc_node.TrackSource.SOURCE_MICROPHONE) {
        microphonePublication = publication;
        break;
      }
    }
    if (!microphonePublication) {
      return;
    }
    if (!microphonePublication.subscribed) {
      microphonePublication.setSubscribed(true);
    }
  }
  #handleTrackSubscription(track, publication, participant) {
    var _a;
    if (publication.source !== import_rtc_node.TrackSource.SOURCE_MICROPHONE || participant.identity !== ((_a = this.linkedParticipant) == null ? void 0 : _a.identity)) {
      return;
    }
    const readAudioStreamTask = async (audioStream) => {
      const bstream = new import_audio.AudioByteStream(
        this.model.sampleRate,
        this.model.numChannels,
        this.model.inFrameSize
      );
      for await (const frame of audioStream) {
        const audioData = frame.data;
        for (const frame2 of bstream.write(audioData.buffer)) {
          this.#session.inputAudioBuffer.append(frame2);
        }
      }
    };
    this.subscribedTrack = track;
    this.readMicroTask = new Promise((resolve, reject) => {
      readAudioStreamTask(new import_rtc_node.AudioStream(track, this.model.sampleRate, this.model.numChannels)).then(resolve).catch(reject);
    });
  }
  #getLocalTrackSid() {
    if (!this.#localTrackSid && this.room && this.room.localParticipant) {
      this.#localTrackSid = (0, import_utils.findMicroTrackId)(this.room, this.room.localParticipant.identity);
    }
    return this.#localTrackSid;
  }
  #publishTranscription(participantIdentity, trackSid, text, isFinal, id) {
    var _a;
    this.#logger.debug(
      `Publishing transcription ${participantIdentity} ${trackSid} ${text} ${isFinal} ${id}`
    );
    if (!((_a = this.room) == null ? void 0 : _a.localParticipant)) {
      this.#logger.error("Room or local participant not set");
      return;
    }
    this.room.localParticipant.publishTranscription({
      participantIdentity,
      trackSid,
      segments: [
        {
          text,
          final: isFinal,
          id,
          startTime: BigInt(0),
          endTime: BigInt(0),
          language: ""
        }
      ]
    });
  }
  #updateState() {
    let newState = "initializing";
    if (this.#pendingFunctionCalls.size > 0) {
      newState = "thinking";
    } else if (this.#speaking) {
      newState = "speaking";
    } else if (this.#started) {
      newState = "listening";
    }
    this.#setState(newState);
  }
  #setState(state) {
    var _a;
    if (((_a = this.room) == null ? void 0 : _a.isConnected) && this.room.localParticipant) {
      const currentState = this.room.localParticipant.attributes[AGENT_STATE_ATTRIBUTE];
      if (currentState !== state) {
        this.room.localParticipant.setAttributes({
          [AGENT_STATE_ATTRIBUTE]: state
        });
        this.#logger.debug(`${AGENT_STATE_ATTRIBUTE}: ${currentState} ->${state}`);
      }
    }
  }
}
// Annotate the CommonJS export names for ESM import in node:
0 && (module.exports = {
  AGENT_STATE_ATTRIBUTE,
  MultimodalAgent,
  RealtimeModel,
  RealtimeSession
});
//# sourceMappingURL=multimodal_agent.cjs.map